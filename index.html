<!DOCTYPE html>
<html lang="en">
	<head>
 		<meta charset="utf-8">
		<title>Harry Potter Project Â· Social Graphs 2017</title>
		<link rel="stylesheet" href="style.css">

		<!-- jquery -->
		<script
			src="https://code.jquery.com/jquery-3.2.1.min.js"
			integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
			crossorigin="anonymous"></script>

		<!-- bxslide -->
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/bxslider/4.2.12/jquery.bxslider.css">
		<script src="https://cdn.jsdelivr.net/bxslider/4.2.12/jquery.bxslider.min.js"></script>

		<!-- fixto -->
		<script src="js/fixto.min.js"></script>
		
		<!-- fancybox -->
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" />
		<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js"></script>

		<!-- <script src="node_modules/jquery-smooth-scroll/jquery.smooth-scroll.min.js"></script> -->
		<script src="js/main.js"></script>
	</head>
	<body>
		<div class="top-bar">
			<div class="overlay"></div>
			<header>
				<h1>Harry Potter Project</h1>
				<h2>Social Graphs</h2>
			</header>
		</div>
		<div class="wrapper">
				<div class="content">
					<div id="sidebar">
						<nav id="sidebar-nav">
							<ul>
								<li class="nav-item"><i class="fa fa-bolt"></i><a href="#introduction">Introduction</a></li>
								<li class="nav-item"><i class="fa fa-bolt"></i><a href="#network">Network analysis</a></li>
								<li class="nav-item"><i class="fa fa-bolt"></i><a href="#text">Text analysis</a></li>
								<li class="nav-item"><i class="fa fa-bolt"></i><a href="#sentiment">Sentiment analysis</a></li>
								<!-- <li><i class="fa-li fa fa-bolt"></i></li> -->
							</ul>
						</nav>
					</div><!--/sidebar-->
					<div id="main">

						<section id="introduction">
							
							<h1>Introduction</h1>

							<p>This website was created in order to present the work done during the Final Project of the course <em>02805 Social Graphs and Interactions</em> at the Technical University of Denmark. 
							The Final Project of the course consists of a building network from a data set chosen by the authors of this website. We choose to analyze the Harry Potter universe and all everything about it. The network was created from the text on the Harry Potter Fandom Wikipedia site <a href="http://harrypotter.wikia.com/wiki/Main_Page">here</a>. The Fandom Wikipedia provides a fully structured copy of all their current pages structured into an XML file. The XML file can be downloaded from <a href="http://s3.amazonaws.com/wikia_xml_dumps/h/ha/harrypotter_pages_current.xml.7z">here</a>.</p>
							
							<p>The network is constructed by finding a limited amount of characters and placing these as "nodes" in the network. Then by extracting the text from each characters Wikipedia website and from this identifying how many other nodes this character links to from its own page. If a link to another character is found then a "edge" (connection) between the two characters is created.</p>

							<p>By continously analyzing each character webpage a network is constructed and basic yet exciting question can be answered; who is the most connected character? Can we find communities between the characters? Which unique words identify each character?</p>

							<p>In the project we also conduct extensive text mining and sentiment analysis of the seven books and eight movie scripts. The structure of the website is setup such that reading from the top to bottom provides a walkthrough of the different analyses performed on the network but the website contains no technical explanation of the procedures, this can be found in the <a href="insertIPythonNotebookHere">IPython Notebook</a></p>

						</section>
						<section id="network">
							
							<h1>Network Analysis</h1>

							<p>The downloaded network can be visualized in different ways like seen below. First a visualization where node size and color is based on the degree. The larger the degree of a node the darker the color and the larger the size.</p>
							<figure>
								<img src="img/basic_network.png">
								<figcaption>Basic network where node color and size is based on degree. Larger node size indicate a higher degree.</figcaption>
							</figure>
							<p> In the project we use six different categories to characterize each character. The categories comprise the following; the four housings, muggles and other wizards, where the latter is a rather large group of characters. Formally this feature is called a the node attribute and by having this type of categorization of the nodes in the network it is possible to conduct community analysis which will be shown later. For now, let's just visualize the network structure as above but using a color scheme based on the categories of each node.</p>
							<figure>
								<img src="img/categorized_network.png">
								<figcaption>The network colored by category/node attribute with size based on degree.</figcaption>
							</figure>
							<p> Continuing inline with the thought process of doing community analysis; it is possible to extract the largest fully connected subcomponent of the network and then plotting this in a community structure as seen below. Here with split the categories evenly on a circle and as is seen the group of 'Other wizards' are quite large as mentioned. The largest fully connected component of the network is termed the giant connected component (gcc) in network theory and by having extracted this the foundation of community analysis have been made. The community analysis will hence be performed on this subgraph in later sections. </p>
							<figure>
								<img src="img/grouped_network.png">
								<figcaption>The giant component of the network (largest fully connected subset of the network) grouped by category</figcaption>
							</figure>
							
							<h2>Basic Statistics</h2>

							<p>Now, investigating the basic statistics of the network to get and idea of the structure. The network consists of 1078 nodes, i.e. characters and include 5211 edges, i.e. connections. As seen from the pictures above we categorize each character into different categories which are; Gyffindors, Ravenclaws, Slytherins, Hufflepuffs, Muggles and Other Wizards,
							where the latter is a fairly large group of characters. These "communities" are what we will refer to as node attributes and further analysis of these communities will be carried out later.
							A fast an easy to calculate statistic is the degree distribution, which is basically found by counting the number of connections each node in the network has and then binning the degrees such that a distribution can be visualized.</p>

							<p>Although first, let us just check the top nodes with respect to degree in the network as seen below.</p>

							<figure><img src="img/degree_table.png" alt="Top 10 nodes sorted by degree"></figure>
							
							<p>Furthermore, in network analysis formally a connection can be categorized into either an in- or out-connection which is refered to as the in- and out-degree of a node. This exists as the graphs created are a type of graph refeered to as a directed graph meaning that the links between characters are directed, i.e. one character might link to another but the other character might not link to the first.</p>

							<p>Hence, by splitting the counts of degrees into in- and out-degrees the following two tables can be made: </p>

							<figure>
								<img src="img/in_degree_table.PNG">
								<figcaption>Top 10 nodes sorted by in-degree</figcaption>
							</figure>
							<figure>
								<img src="img/out_degree_table.PNG">
								<figcaption>Top 10 nodes sorted by out-degree</figcaption>
							</figure>
							
							<h2>Degree Distribution</h2>

							<p>From the above analysis of the node degrees, it is possible to visualize the complete degree distribution instead of just showing the top 10 nodes by binning the number of degrees and plotting this.
							Hence, below is the in- and out-degree distribution of the network where both linear axes and log-log axes are used to show the distribution. What is interesting about using the log-log axis is that the distributions approximately form a straight line with some slope meaning that the degree distributions follow a power law.</p>
							<p>This trait is very characteristic for social networks which have been investigated by researchers before and is a consequence of the fact that some nodes tend to have a very large degree and acting as hubs in the network and in this way creating the power law distribution.</p>

							<figure>
								<img src="img/in_out_degree.png">
								<figcaption>In- and out-degree of the the network plotted on linear and log-log axes</figcaption>
							</figure>
							
							<h2>Centrality</h2>
							<p> Centrality of nodes is a measure of how central a node is in a network. Different measures of centrality exists but here we just present two different measures; betweenness and eigenvector centrality. In layman terms the betweenness centrality is a measure of the fraction of how many of the shortest paths through the network that passes through that specific node. The shortest path between two nodes are simply defined as the path that would take you from node a to b in the shortest amount of steps. Hence a high betweenness centrality of a node usually means that the node is well connected as will be seen when computing the centrality measures. Eigenvector centrality is based on a different idea as a high node degree does not necessarily mean that the node has a high eigenvector centrality. The scoring of eigenvector centrality is basically controlled having node neighbors that also have a high eigenvector centrality and hence if one node has many links to important nodes in the network, then that node has a high eigenvector centrality.</p> 
							
							<p> Now, computing the two centrality measures as seen in the below tables, it is seen that there is quite some correspondance between the two tables although the relative scoring between the two is different. It is quite clear that Harry Potter is the most important character in the network, which was expected as both the degree is high and intuitively one would know that he connects and interacts with a lot of different people in the network. From the betweenness centrality it is seen that some of the nodes with a high score here is not present in the top 10 eigenvector centrality table indicating that these were not connected to important characters in the network but were merely good links for shortest paths in the network. Generally it seems that the main characters of the books and movies have a high eigenvector centrality but this is not necessarily true for the betweenness centrality.</p>
							<figure>
								<img src="img/betweenness_table.PNG">
								<figcaption>Top 10 nodes sorted by betweenness centrality</figcaption>
							</figure>
							<figure>
								<img src="img/eigenvector_centrality.PNG">
								<figcaption>Top 10 nodes sorted by eigenvector centrality</figcaption>
							</figure>
							
							<h2>Community Analysis</h2>
								<p> As mentioned the six categories which all nodes of the network was categorized into lay the foundation of some interesting community analysis. The following analysis will be made only on the giant connected component (GCC) which was visualized above. The GCC consists of 865 nodes and 3450 edges and hence includes most of the node in the network which had a total of 1078 nodes. In the analysis we make use of a measure called the modularity which gives a score to the current partioning of the network regarding how community like the partioning is. It can be thought of as a way of analyzing potential communities by comparing a potential community to the community that could be formed if randomness was introduced into the same network. If  the modularity is positive the potential community has more links than expected when comparing to a random network. When the modularity is zero then the community links are fully explained by the random variable determining the underlying links (i.e. fully explained by the link distribution which was shown to follow a power law above) and if  the modularity is negative there is less links in the community than when compared to random. Then the overall modularity is the average of the modularities of each community and hence provides a good estimate of the community structure of the partitions. </p>

								<p>The calculated modularity of the current six partitions in the GCC as found to be 0.19 which is rather lower but still shows that the categories provide some community structure. If one was to exclude the whole category called 'Other wizards' and only include the four housings and muggles as categories, then probably the modularity would be higher. The details of the calculation of the modularity of the current partioning can be found in the IPython Notebook. </p>

								<p>It is of interest now to analyze the network with a community analysis algorithm with the goal of finding partitions between nodes such as the modularity is maximized. This can be done by using the implementation of the Louvain algorithm in Python. By using this algorithm 20 different partitions are found and a modularity of 0.48 is obtained. Hence to obtain a larger modularity he algorithm decided to use a lot more partitions than just the 6 found by categorizing as mentioned. It is of interest to visualize how these 20 new partitions compare to the 6 partitions which we term the 'ground truth'. If one of the partitions correspond well to the ground truth partition then this can be visualized from the below network plot. In this plot we plot the 20 communities evenly distributed on the perimeter of a circle and keep the coloring scheme which was based on the categories assigned to the nodes.</p>

								<figure>
									<img src="img/louvain_network.png">
									<figcaption>Visualization of the 20 partitions found by the Louvain algorithm using the original coloring scheme which was based on the 6 categories or the 'ground truth'.</figcaption>
								</figure>		
							
								<p>To investigate the partitions further a so called confusion matrix can be formed. Such a matrix (or table) is constructed such that the original partitions are set as rows and the found Louvain partitions are set as columns. Then by finding intersections between the original communities and the 20 partitions found by the algorithm, it is easily seen if some of the 20 partitions resemble the original partitioning. The confusion matrix is seen below.</p>
										
								<figure>
									<img src="img/confusion_matrix.PNG">
									<figcaption>The confusion matrix created by comparing the original six communities with the 20 communities which was found by the Louvain algorithm.</figcaption>
								</figure>
								
								<p>Taking a look at the confusion matrix, it seems that partition number 7 mostly includes muggles and hence it will be of interest to analyze that specific community. Below we print the list of characters belonging to community number 7. For a person with domain knowledge of the Harry Potter universe it is easily seen why this community is created. The community mostly consist of the Dursleys which is the family which took care of Harry Potter when he was a child and furthermore it consists of Harry's parents and some of their acquitances. Harry's mother was related to Petunia Dursley and hence it is expected that the they are categorized into this community.</p>
												
								<figure>
									<img src="img/partition_7.PNG">
									<figcaption>Characters included in the seventh partition created by the Louvain algorithm.</figcaption>
								</figure>

						</section>
						<section id="text">
							
							<h1>Natural language processing and text mining</h1>

							<p>Now, we dwelve into some natural language processing of all the text provided from the Wikipedia sites, the books and the movie scripts. The links to the books and movie scripts are found in the IPython notebook. An immediate interest of is to analyze what words are unique to the most important characters in the network. This can be done by using the term-frequency and inverse document frequency scoring scheme which will be refereed to as the TF-IDF score. This scoring scheme is based on an analysis of a corpus of documents (collection of texts) where first the frequency of a word within a document is found and then the frequency of that word across the corpus of documents is found. In layman terms the TF-IDF score can be understood by considering the limits of the score; if a word is found with a high frequency in a single document and only exist in that document across the corpus of documents, then it attains a maximum TF-IDF score. If another word appears with a high frequency in a document but is present in all other documents of the corpus then it attains a TF-IDF score of zero as the word is not a unique identifier of that specific document. </p>

							<p>By computing the TF-IDF scores the results can be visualized in so called WordClouds where words with a high TF-IDF score is highlighted. We intend to analyze three different corpus of documents in the following sections; Wikipedia texts from the nine most important characters, the seven different books and the eight different movies and from this find the words with the highest TF-IDF score. </p>

							<h2>TF-IDF score and WordClouds</h2>
								<h3>Analyzing the top nine characters Wikipedia page </h3>
								We extract the nine most important characters based on their total node degree corresponding. These were shown in the above table of total node degree. By tokenizing and cleaning the text from the characters Wikipedia site the following WordClouds are obtained.

								

							<p>Blablabla</p>

							<h2>Another subsection</h2>

							<p>Text text</p>

							<div class="slider">
								<div>I am a slide.</div>
								<div>I am another slide.</div>
							</div>

						</section>
						<section id="sentiment">
							
							<h1>Sentiment analysis</h1>

							<p>Blablablabla</p>

							<h2>Subsection title</h2>

							<p>Blablabla</p>

							<h2>Another subsection</h2>

							<p>Text text</p>

						</section>
					</div><!--/main-->
					
				</div><!--/content-->
				<footer id="colophon">
					<p>Project by Athina Tsagkari, Terkel Bo Olsen &amp; Jan Szwagierczak. Last update: 29 Nov 2017</p>
				</footer>
		</div><!--/wrapper-->
	</body>
</html>
